import argparse
import pickle
import pandas as pd
import numpy as np
from nltk import FreqDist
from keras.preprocessing.text import text_to_word_sequence
from keras.models import load_model
from keras.layers import Embedding, LSTM, Dense, Input

def preprocess_data_test(train_file,i_to_w):

    # Read the train set
    data_train = pd.read_csv(train_file, header=0)
    X_train = data_train.MR.tolist() # convert rm part to a list

    X_train_seq,dico_ = mr2oh(X_train) # convert train set into list of slots and values.
    X_train = [proc_text(y) for y in X_train_seq] # process text

    w_to_i={word: idx for idx, word in enumerate(i_to_w)} # dictionary that converts words to their corresponding index
    
    X_train_oh=ref2oh(X_train,w_to_i) # convert words in ref sentences into their indexes

    return np.array(X_train_oh), dico_


def mr2oh(data):

    # create sequences of w2v representation from MR column
    seq = []
    dictio=[] # will contain an array of dictionaries for each rm, with rm slots and their values in each dictionary

    # Take each row of the rm part
    for mr in data:
        chunks =''
        dic={} # dictionary containing slots and their values in the rm
        # Separate each word in the rm
        for s in mr.split(','):
            s=s.strip()
            separator = s.find('[')
            chunk = s[:separator]
            if chunk == "customer rating":
                chunk= "<customer_rating>"
            else:
                chunk='<'+chunk+'>' # add signs before and after slots to identify the slots
            chunks+=' '+chunk

            value = s[separator + 1:-1] # value of each slot of the rm
            dic[chunk]=value 
            chunks+=' '+value
        seq.append(chunks) # add the chunks list to the seq array
        dictio.append(dic) # add the dictionary with slot and values for the rm to dictio
            
    return seq,dictio


def ref2oh(data, ref_word2idx):
    seq_oh = []

    for i in range(len(data)):
        seq=[]
        for j in data[i]:
            if j in ref_word2idx:
                seq.append(ref_word2idx[j]) # convert the words into their corresponding index
            else:
                seq.append(0)
        seq_oh.append(seq) 

    return seq_oh


def proc_text(ref, full_stop=True):
    ref ='<go> '+ref
    # process ref
    chars_to_filter = '!"#$%&()*+,-/:;=?@[\\]^`{|}~\t\n '
    if full_stop:
        ref = ref.replace('.', ' . ')

        # Remove punctuations from the text, with only words remaining:
        return text_to_word_sequence(ref, filters=chars_to_filter)

    else:

        return text_to_word_sequence(ref, filters=chars_to_filter)

# Define the arguments in the commands when running
parser = argparse.ArgumentParser(description='Machine Reading test file')
parser.add_argument('--test_dataset',help='CSV file containing test dataset',required=True)
parser.add_argument('--output_test_file',help='Path to output generated by model, txt file',
    required=True)
parser.add_argument('--lstm_model',help='Path to previously stored model, .h5 model', 
    required = True)


def main():
    args = parser.parse_args()

    # Load the stored model
    lstm_path = args.lstm_model
    if '.h5' not in lstm_path:
        lstm_path=lstm_path+'.h5'
    lstm = load_model(lstm_path)

    # Load the id2word dictionary
    id2word_path = 'dico_'+lstm_path
    id2word_file = pickle.load(open(id2word_path,'rb'))
    
    # Load the test file
    x_test, dico_file = preprocess_data_test(args.test_dataset,id2word_file)

    # Function to turn prediction into word
    def decode_sequence_2(input_seq,model,y_idx2word):
        start_seq=int(y_idx2word.index('<go>'))
        target_seq = [start_seq]#initialize the sequence with the start sequence word index
        stop_condition = False
        decoded_sentence = ''
        while not stop_condition:
            output_tokens=model.predict([input_seq,np.array([target_seq],ndmin=2)])
            sampled_token_index = np.argmax(output_tokens[0,-1, :]) # predict next index
            sampled_char = y_idx2word[sampled_token_index] # convert this index to corresponding word
            decoded_sentence += sampled_char+' ' # add the word to the decoded sentence
            target_seq.append(sampled_token_index)
            if (sampled_char == '.' or  len(decoded_sentence.split()) > 30):
                stop_condition = True
        return decoded_sentence

    output_test_file = open(args.output_test_file,'w')
    
    # Loop to turn predictions back to sentences
    for k in range(x_test.shape[0]):
        # sent=' '.join(y_idx2word[i] for i in x_train[k])
        # print('MR : ',sent)
        s=decode_sequence_2(np.array([x_test[k]]),lstm,id2word_file) # decode the sentence
        for key,value in dico_file[k].items():
            key=key.lower()
            s=s.replace(key,value) # replace the slots by their values or their nearest value
            if key=='<area>' and '<near>' in s and '<near>' not in dico_file[k]:
                s=s.replace('<near>',value)
            elif key=='<near>' and '<area>' in s and '<area>' not in dico_file[k]:
                s=s.replace('<area>',value)
        output_test_file.write(s + '\n')
    print("Check file with name", args.output_test_file,'in your cd.')
    output_test_file.close()

if __name__ == '__main__':
    main()




